{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e18f229-9779-4c31-acd4-f65a74f9aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc785ede-8d46-4f19-b6a4-ede605678525",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ec46d5-a918-4aa1-840b-928a0a1a81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e11f57d-b67f-4565-a5c9-4118907cd49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36223565-9d97-4ac9-b50a-aea77400a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a04b4ad-3bc8-404f-a3a1-0581b28f88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "772a0f5d-e8bd-4108-ad3e-56eef0b353e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35bbc663-2135-47f1-90e9-af37e0cbeddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8d7852d-9e5e-4ef5-a5b7-d9a0c1cc38ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([639.,  11.,   6.,  11.,   6.,   9.,  11.,  12.,  11.,  68.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344e84a-ab5a-4095-b079-e314aee45855",
   "metadata": {},
   "source": [
    "# Net 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04a3677-1b9b-43cf-adc4-2821d93bb30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_init(n_input_neurons, n_output_neurons):\n",
    "    \"\"\"\n",
    "    He 초기화를 적용하여 가중치를 초기화하는 함수입니다.\n",
    "    :param n_input_neurons: 입력 뉴런의 개수\n",
    "    :param n_output_neurons: 출력 뉴런의 개수\n",
    "    :return: 초기화된 가중치 행렬\n",
    "    \"\"\"\n",
    "    std = np.sqrt(2 / n_input_neurons)\n",
    "    weights = np.random.randn(n_input_neurons, n_output_neurons) * std\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "024d3ea6-55fb-451a-8ede-09d6b69246cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out   \n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ad3fe3a-f0a5-4756-a04e-35b65a98b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4662a490-dd9d-4558-93be-41510158edb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6187447c-4281-4549-887f-c7991c8ae2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92cc4a54-bf1e-4845-8dd7-13f2606e0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d11f69e3-c5ba-401d-94c4-ad419ec9196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \n",
    "        # 가중치 초기화 \n",
    "        self.params = {}\n",
    "        self.params['W1'] = he_init(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = he_init(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['ReLU1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ba9c6ae-3672-4ec9-8d76-15d02917f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNetExtend:\n",
    "    \"\"\"완전 연결 다층 신경망(확장판)\n",
    "    가중치 감소, 드롭아웃, 배치 정규화 구현\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\n",
    "    use_dropout : 드롭아웃 사용 여부\n",
    "    dropout_ration : 드롭아웃 비율\n",
    "    use_batchNorm : 배치 정규화 사용 여부\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size_list, output_size,\n",
    "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0, \n",
    "                 use_dropout = False, dropout_ration = 0.5, use_batchnorm=False):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.use_dropout = use_dropout\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.params = {}\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.__init_weight(weight_init_std)\n",
    "\n",
    "        # 계층 생성\n",
    "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
    "        self.layers = OrderedDict()\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "                                                      self.params['b' + str(idx)])\n",
    "            if self.use_batchnorm:\n",
    "                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\n",
    "                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx-1])\n",
    "                self.layers['BatchNorm' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n",
    "                \n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "            \n",
    "            if self.use_dropout:\n",
    "                self.layers['Dropout' + str(idx)] = Dropout(dropout_ration)\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def __init_weight(self, weight_init_std):\n",
    "        \"\"\"가중치 초기화\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "        \"\"\"\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLUを使う場合に推奨される初期値\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoidを使う場合に推奨される初期値\n",
    "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key or \"BatchNorm\" in key:\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t, train_flg=False):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블 \n",
    "        \"\"\"\n",
    "        y = self.predict(x, train_flg)\n",
    "\n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "\n",
    "        return self.last_layer.forward(y, t) + weight_decay\n",
    "\n",
    "    def accuracy(self, X, T):\n",
    "        Y = self.predict(X, train_flg=False)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        if T.ndim != 1 : T = np.argmax(T, axis=1)\n",
    "\n",
    "        accuracy = np.sum(Y == T) / float(X.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, X, T):\n",
    "        \"\"\"기울기를 구한다(수치 미분).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_W = lambda W: self.loss(X, T, train_flg=True)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
    "            \n",
    "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
    "                grads['gamma' + str(idx)] = numerical_gradient(loss_W, self.params['gamma' + str(idx)])\n",
    "                grads['beta' + str(idx)] = numerical_gradient(loss_W, self.params['beta' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t, train_flg=True)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
    "                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\n",
    "                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81b9f446-f32c-4924-a0f2-13c59d9c9f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c23e536-fe85-47a0-a4f6-2868d12a258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultiLayerNetExtend(784, [100,100,100,100], 10, activation='relu', weight_init_std='relu', weight_decay_lambda=0, \n",
    "                 use_dropout = False, dropout_ration = 0.5, use_batchnorm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8091576e-5b18-414e-99b7-859ee8ce774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c0512f3-2c74-4520-974d-2ea3e9a52ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] \n",
    "\n",
    "\n",
    "class Momentum:\n",
    "\n",
    "    \"\"\"모멘텀 SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "\n",
    "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
    "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "\n",
    "    \"\"\"RMSprop\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd5e6ae1-d600-42de-bd93-c8feeb9ebe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1e85f1c-b27d-4e4f-af4a-bf7535e9a238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11821666666666666 0.1146\n",
      "0.8953333333333333 0.8987\n",
      "0.92015 0.9187\n",
      "0.93245 0.9326\n",
      "0.9423333333333334 0.9401\n",
      "0.9468166666666666 0.9436\n",
      "0.9527333333333333 0.9463\n",
      "0.9556666666666667 0.9501\n",
      "0.9604166666666667 0.953\n",
      "0.9630833333333333 0.9544\n",
      "0.9649333333333333 0.9573\n",
      "0.9672166666666666 0.9592\n",
      "0.9695 0.9606\n",
      "0.9684166666666667 0.9597\n",
      "0.9732666666666666 0.9646\n",
      "0.9725 0.9632\n",
      "0.9751666666666666 0.9649\n"
     ]
    }
   ],
   "source": [
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    grad = net.gradient(x_batch, y_batch)\n",
    "    params = net.params\n",
    "    \n",
    "    # 갱신\n",
    "    # for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "    #     net.params[key] -= learning_rate * grad[key]\n",
    "    optimizer.update(params, grad)\n",
    "\n",
    "    loss = net.loss(x_batch, y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = net.accuracy(x_train, y_train)\n",
    "        test_acc = net.accuracy(x_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a1322-0a86-48f3-9876-5f350bbcc126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
